{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DecTreeAssign1_Frank.ipynb","provenance":[{"file_id":"1bjGYSdOyVTOAVgFiAwIJhhxCPujoPgD7","timestamp":1631466848524}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0AUMIs3ogI2u"},"source":["For this program, you will be coding some of the ideas around decision trees. In particular, you will be looking into splitting the root node for a tree. Data is provided via an input file. **Be sure to document your code.**\n","\n","**You may not use an packages or \"canned\" code in your program other than simple function calls to needed math functions or file I/O**\n","\n","The input file is a csv file named *DecTreeAssign1.dat* and is located in the Data folder on Google.\n","\n","The first line contains the class name followed by the n feature names. Your program should allow any number of features >= 1. I.e., the number of features should not be hardcoded. Neither should the number of observations.\n","\n","Each subsequent line is an observation that has the class indicator, \"M\" or \"B\", followed by numeric values for each of the n features.\n","\n","Read the input file and build a data matrix such that the feature values are scaled in the interval [0, 1]. Calculate  the mean value for each feature after they are scaled.\n","\n","You will now need to do some calculations for splitting the root node.\n","\n","For the root node, print the following values to the console:\n","* Number of class \"B\" observations\n","* Number of class \"M\" observations\n","* Total number of observations\n","* Gini index for the root\n","* Entropy for the root\n","\n","Here is an example console output:\n","```\n","Class B observations:   262\n","Class M observations:   198\n","Total observations:     460\n","Root gini:    0.4932\n","Root entropy: 0.9860\n","```\n","\n","For each feature:\n","\n","Calculate the Gini index and Entropy for each child node if a split is made on the mean value for the feature. Calculate the combined Gini index and Entropy for the child nodes. Child 1 is the < decision, and Child 2 is the >= decision.\n","\n","Students should write their own Gini and Entropy functions.\n","\n","Open a csv output file named: *DecTreeAssign1_out.csv*\n","\n","Write one line to the output file for each feature that contains the following separated by commas:\n","\n","* Feature name\n","* Feature mean value after scaling\n","* Child 1 count for class \"B\"\n","* Child 1 count for class \"M\"\n","* Child 1 Gini\n","* Child 1 Entropy\n","* Child 2 count for class \"B\"\n","* Child 2 count for class \"M\"\n","* Child 2 Gini\n","* Child 2 Entropy\n","* Combined Gini for Child 1 and Child 2\n","* Combined Entropy for Child 1 and Child 2\n","\n","For readibility in my output file, all floats written were limited to 4 decimal places.\n","\n","Here is an example of a possible first 4 lines of the output file (these are not necessarily what you will get, just an example).\n","```\n","F00, 0.3382,  311,   32, 0.1692, 0.7290,   46,  180, 0.3242, 0.7290, 0.2308, 0.5592\n","F01, 0.3240,  254,   52, 0.2821, 0.9658,  103,  160, 0.4765, 0.9658, 0.3720, 0.8001\n","F02, 0.3329,  313,   30, 0.1596, 0.7112,   44,  182, 0.3136, 0.7112, 0.2208, 0.5404\n","F03, 0.2169,  324,   41, 0.1994, 0.6385,   33,  171, 0.2712, 0.6385, 0.2252, 0.5541\n","```\n","\n","Load the output file into Google Sheets, Excel or some other spreadsheet program. Determine which feature should be chosen to split on at the root using the combined Gini and Entropy criteria. Is it the same for both?\n","\n","If you are interested, feel free to expand your program to do more. Be sure to document your code.\n"]},{"cell_type":"code","metadata":{"id":"bzTegD_-gCYt"},"source":["'''\n","Functions and imports for Decision Tree Assignment #1\n","'''\n","\n","import  math\n","\n","'''\n","this function scale the numeric features into range [0,1]\n","argument list:\n","- feature: an array(vector) of the numeric feature\n","'''\n","def scale_feature(feature):\n","  max = float('-inf')\n","  min = float('inf')\n","  # first loop find out the max and the min value of feature array\n","  for i in range(len(feature)):\n","    if feature[i] > max:\n","      max = feature[i]\n","    if feature[i] < min:\n","      min = feature[i]\n","  # second loop scale each element of the feature\n","  for i in range(len(feature)):\n","    feature[i] = (feature[i] - min)/(max - min)\n","  return feature\n","\n","\n","'''\n","this function calculate the gini index for a node\n","it takes in the number of different labels as arguments\n","'''\n","def gini_index(num_M, num_B):\n","  if num_M == 0 and num_B == 0:\n","    return 0\n","  total = num_B + num_M\n","  gini = 1 - ((num_B/total)**2 + (num_M/total)**2)\n","  return gini\n","\n","'''\n","this function calculate the entropy for a node\n","it takes in the number of different labels as arguments\n","'''\n","def entropy(num_M, num_B):\n","  total = num_M + num_B\n","  if (num_M == 0 or num_B == 0):  #pure node\n","    entropy = 0\n","  else:\n","    entropy = -((num_M/total)*math.log2(num_M/total) + (num_B/total)*math.log2(num_B/total))\n","  return entropy\n","\n","'''\n","this function calculate the mean of a feature\n","it takes in the feature vector as an argument\n","'''\n","def cal_mean(feature):\n","  sum = 0\n","  for i in range(len(feature)):\n","    sum += feature[i]\n","  return sum/len(feature)\n","\n","\"\"\"\n","This function calculate the optimal split for one feature\n","\"\"\"\n","def find_optimal_split(feature, data_matrix):\n","   # loop through each possible spliting point\n","   global_minimum_gini = 999\n","   global_entropy = 0\n","   feature_index = 999\n","   for i in range(len(feature)):\n","     splitting_point = feature[i]\n","     m_count_cld1 = 0\n","     m_count_cld2 = 0\n","     b_count_cld1 = 0\n","     b_count_cld2 = 0\n","     for m in range(len(feature)):\n","       if feature[m] < splitting_point: # belongs to child 1\n","         if data_matrix[m][0] == 'M':\n","           m_count_cld1 += 1\n","         else:\n","           b_count_cld1 += 1\n","       else: # child 2 (>= average)\n","         if data_matrix[m][0] == 'M':\n","           m_count_cld2 += 1\n","         else:\n","           b_count_cld2 += 1\n","     # calculate necessary index and write into output file\n","     gini_cld1 = gini_index(b_count_cld1,m_count_cld1)\n","     gini_cld2 = gini_index(b_count_cld2,m_count_cld2)\n","     weighted_gini = (b_count_cld1 + m_count_cld1)/len(data_matrix)*gini_cld1 + (b_count_cld2 + m_count_cld2)/len(data_matrix)*gini_cld2\n","     entropy_cld1 = entropy(b_count_cld1,m_count_cld1)\n","     entropy_cld2 = entropy(b_count_cld2,m_count_cld2)\n","     weighted_entropy = (b_count_cld1 + m_count_cld1)/len(data_matrix)*entropy_cld1 + (b_count_cld2 + m_count_cld2)/len(data_matrix)*entropy_cld2\n","     if weighted_gini < global_minimum_gini:\n","       global_minimum_gini = weighted_gini\n","       global_entropy = weighted_entropy\n","       feature_index = i\n","   #print(\"gini is {}, entropy is {}, at {}'s feature, with split point of {}\".format(global_minimum_gini, global_entropy, feature_index, feature[i]))\n","   #print(\"chl1_m {} child1_b {} child2_m {} child2_b {}\".format(m_count_cld1, b_count_cld1, m_count_cld2, b_count_cld2))\n","   return global_minimum_gini, global_entropy, feature_index\n","\n","def calculate_performance(actual, predict):\n","  pass #return the confusion matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"clcN-h1ggSn_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632876978142,"user_tz":240,"elapsed":268,"user":{"displayName":"Frank Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj06dBZfud6DMRGM2JZPzwnvHcRH4BvrEF-qGYV-g=s64","userId":"10750098987506420755"}},"outputId":"2ca84258-2c06-4a60-b0a8-ed6e28281436"},"source":["'''\n","Decision Tree Assignment #1\n","'''\n","\n","if __name__ == \"__main__\":\n","  \"\"\"\n","  step 1:Read the input file and build a data matrix such that the feature values are scaled in the interval [0, 1]\n","  \"\"\"\n","  #Data file name\n","  data_file_name = \"DecTreeAssign1.dat\"\n","  data_matrix = []\n","  # read the file\n","  with open(data_file_name, 'r') as data_file_ptr:\n","    for index, item in enumerate(data_file_ptr):\n","      item_list = item.split(',')\n","      if index != 0:  #first row of the data is the title\n","        data_matrix.append(item_list) #this is the data matrix\n","      else:\n","        feature_name = item_list\n","      \n","  #get rid of the new line character in last feature name\n","  feature_name[len(feature_name)-1] = \"F{}\".format(len(feature_name)-2)\n","\n","  #normalize each column of the matrix\n","  for i in range(1, len(data_matrix[0])): # loop for each column/feature\n","    feature_vector = [data_matrix[j][i] for j in range(len(data_matrix))]\n","    for k in range(len(feature_vector)):\n","      feature_vector[k] = float(feature_vector[k]) #covert to float\n","    feature_vector = scale_feature(feature_vector)\n","    for m in range(len(data_matrix)):\n","      data_matrix[m][i] = feature_vector[m]\n","\n","  \"\"\"\n","  step 2: print necessary info for root node\n","  \"\"\"\n","  b_count = 0\n","  m_count = 0\n","  for i in range(len(data_matrix)):\n","    if data_matrix[i][0] == 'B':\n","      b_count += 1\n","    else:\n","      m_count += 1\n","  print(\"Class B observations: \" + str(b_count))\n","  print(\"Class M observations: \" + str(m_count))\n","  print(\"Total observations: \" + str(len(data_matrix)))\n","  print(\"Root gini: \" + str(gini_index(b_count,m_count)))\n","  print(\"Root entropy: \" + str(entropy(b_count,m_count)))   \n","  print(\"\\n\\n\")\n","\n","  \"\"\"\n","  step 3:\n","  For each feature: Calculate the Gini index and Entropy for each child node if a split \n","  is made on the mean value for the feature. \n","  Calculate the combined Gini index and Entropy for the child nodes. \n","  Child 1 is the < decision, and Child 2 is the >= decision.\n","  \"\"\"\n","\n","  mean_feature = []\n","  # initialize the output file\n","  out_file_name = \"DecTreeAssign1_out.csv\"\n","  with open(out_file_name, 'w') as out_file_ptr:\n","    for i in range(1, len(data_matrix[0])): # loop for each column/feature\n","      # extract the feature out\n","      feature_vector = [data_matrix[j][i] for j in range(len(data_matrix))]\n","      for k in range(len(feature_vector)):\n","        feature_vector[k] = float(feature_vector[k])\n","      average = cal_mean(feature_vector)\n","\n","      m_count_cld1 = 0\n","      m_count_cld2 = 0\n","      b_count_cld1 = 0\n","      b_count_cld2 = 0\n","      # split the feature based on mean\n","      for m in range(len(feature_vector)):\n","        if feature_vector[m] < average: # belongs to child 1\n","          if data_matrix[m][0] == 'M':\n","            m_count_cld1 += 1\n","          else:\n","            b_count_cld1 += 1\n","        else: # child 2 (>= average)\n","          if data_matrix[m][0] == 'M':\n","            m_count_cld2 += 1\n","          else:\n","            b_count_cld2 += 1\n","      # calculate necessary index and write into output file\n","      gini_cld1 = gini_index(b_count_cld1,m_count_cld1)\n","      entropy_cld1 = entropy(b_count_cld1,m_count_cld1)\n","      gini_cld2 = gini_index(b_count_cld2,m_count_cld2)\n","      entropy_cld2 = entropy(b_count_cld2,m_count_cld2)\n","      weighted_gini = (b_count_cld1 + m_count_cld1)/len(data_matrix)*gini_cld1 + (b_count_cld2 + m_count_cld2)/len(data_matrix)*gini_cld2\n","      weighted_entropy = (b_count_cld1 + m_count_cld1)/len(data_matrix)*entropy_cld1 + (b_count_cld2 + m_count_cld2)/len(data_matrix)*entropy_cld2\n","      out_file_ptr.write(\"{}, {:0.4f}, {}, {}, {:0.4f}, {:0.4f}, {}, {}, {:0.4f}, {:0.4f}, {:0.4f}, {:0.4f}\\n\".format(feature_name[i], average, b_count_cld1, m_count_cld1, gini_cld1, entropy_cld1, b_count_cld2, m_count_cld2, gini_cld2, entropy_cld2, weighted_gini, weighted_entropy))\n","\n","\n","  \"\"\"\n","  for item in data_matrix:\n","    print(item)\n","  \"\"\"\n","  \n","  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Class B observations: 357\n","Class M observations: 212\n","Total observations: 569\n","Root gini: 0.4675300607546925\n","Root entropy: 0.9526351224018599\n","\n","\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"bpOaSXsACEsC"},"source":["Optimal Split"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uOjEQAaYCFgC","executionInfo":{"status":"ok","timestamp":1632877890994,"user_tz":240,"elapsed":2016,"user":{"displayName":"Frank Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj06dBZfud6DMRGM2JZPzwnvHcRH4BvrEF-qGYV-g=s64","userId":"10750098987506420755"}},"outputId":"7a224f5a-406f-452a-c962-5645f4da2dd2"},"source":["for i in range(1, len(data_matrix[0])): # loop for each column/feature\n","  # extract the feature out\n","  feature_vector = [data_matrix[j][i] for j in range(len(data_matrix))]\n","  glob_gini, glob_entropy, glob_index = find_optimal_split(feature_vector, data_matrix)\n","  print(\"for feature {}, combined_gini is {}, combined_entropy is {}, with {}'s feature as splitting point with value of {}\".format(feature_name[i], glob_gini, glob_entropy, glob_index, data_matrix[index][i]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["for feature F00, combined_gini is 0.19242486589915644, combined_entropy is 0.4896488694028093, with 514's feature as splitting point with value of 0.3818921860949407\n","for feature F01, combined_gini is 0.3690697283638583, combined_entropy is 0.7961722035638179, with 131's feature as splitting point with value of 0.3165370307744335\n","for feature F02, combined_gini is 0.1916093835208614, combined_entropy is 0.4860075931192521, with 205's feature as splitting point with value of 0.3694976159214982\n","for feature F03, combined_gini is 0.1867183670721237, combined_entropy is 0.4778349162125473, with 38's feature as splitting point with value of 0.23686108165429479\n","for feature F04, combined_gini is 0.41023756912945447, combined_entropy is 0.8555307306582929, with 444's feature as splitting point with value of 0.3567753001715266\n","for feature F05, combined_gini is 0.30446499363426294, combined_entropy is 0.6855696883147584, with 23's feature as splitting point with value of 0.20425127292804127\n","for feature F06, combined_gini is 0.20038987504140576, combined_entropy is 0.5057017334259268, with 7's feature as splitting point with value of 0.17539831302717898\n","for feature F07, combined_gini is 0.15216250204290344, combined_entropy is 0.40684425572913385, with 31's feature as splitting point with value of 0.21545725646123262\n","for feature F08, combined_gini is 0.4237271089506236, combined_entropy is 0.8815560851240232, with 39's feature as splitting point with value of 0.253030303030303\n","for feature F09, combined_gini is 0.4535563343074385, combined_entropy is 0.9318999320629093, with 477's feature as splitting point with value of 0.19355518112889644\n","for feature F10, combined_gini is 0.2870251567472569, combined_entropy is 0.6657385945956105, with 264's feature as splitting point with value of 0.09940249864204237\n","for feature F11, combined_gini is 0.45876354167039873, combined_entropy is 0.9362890179154333, with 317's feature as splitting point with value of 0.18515735502121639\n","for feature F12, combined_gini is 0.2869539890492152, combined_entropy is 0.6639712575909122, with 129's feature as splitting point with value of 0.08825331008811195\n","for feature F13, combined_gini is 0.2164295292244814, combined_entropy is 0.5365563652016728, with 43's feature as splitting point with value of 0.05918587667492221\n","for feature F14, combined_gini is 0.4597170733133412, combined_entropy is 0.9391573741122083, with 358's feature as splitting point with value of 0.11010640106061119\n","for feature F15, combined_gini is 0.3999663299885008, combined_entropy is 0.8428595394508551, with 87's feature as splitting point with value of 0.10550665424940667\n","for feature F16, combined_gini is 0.35465550596918793, combined_entropy is 0.75919271084204, with 189's feature as splitting point with value of 0.07492424242424242\n","for feature F17, combined_gini is 0.3703371923546559, combined_entropy is 0.7986228674559537, with 487's feature as splitting point with value of 0.17849971585527566\n","for feature F18, combined_gini is 0.453358983760673, combined_entropy is 0.9298507265069735, with 12's feature as splitting point with value of 0.051190409185568754\n","for feature F19, combined_gini is 0.4453560433721948, combined_entropy is 0.9180125756954363, with 339's feature as splitting point with value of 0.02843994859251275\n","for feature F20, combined_gini is 0.14231918091829165, combined_entropy is 0.3906922696191142, with 31's feature as splitting point with value of 0.34329420135183203\n","for feature F21, combined_gini is 0.37312622957188357, combined_entropy is 0.7970190163306347, with 230's feature as splitting point with value of 0.4275053304904051\n","for feature F22, combined_gini is 0.14554606170427217, combined_entropy is 0.39064823727530884, with 486's feature as splitting point with value of 0.31570297325564023\n","for feature F23, combined_gini is 0.14447677012193813, combined_entropy is 0.3924740276719886, with 64's feature as splitting point with value of 0.19214510420762876\n","for feature F24, combined_gini is 0.3887390314744099, combined_entropy is 0.8291030803342945, with 368's feature as splitting point with value of 0.35283629399722644\n","for feature F25, combined_gini is 0.31432263623494455, combined_entropy is 0.7114050124382566, with 29's feature as splitting point with value of 0.17736317683926614\n","for feature F26, combined_gini is 0.23227683170716923, combined_entropy is 0.552292476536513, with 321's feature as splitting point with value of 0.22891373801916934\n","for feature F27, combined_gini is 0.148302190991341, combined_entropy is 0.4035625279471552, with 41's feature as splitting point with value of 0.3848797250859107\n","for feature F28, combined_gini is 0.3978579904766197, combined_entropy is 0.8428463808623464, with 117's feature as splitting point with value of 0.14133648728562978\n","for feature F29, combined_gini is 0.4179400427686192, combined_entropy is 0.8779664685160286, with 87's feature as splitting point with value of 0.09510691328873153\n"]}]}]}